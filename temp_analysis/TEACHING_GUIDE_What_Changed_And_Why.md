# Complete Guide: Understanding What Changed and Why

## Table of Contents
1. [What Your Data Actually Is](#what-your-data-actually-is)
2. [The Problem We Fixed](#the-problem-we-fixed)
3. [Exact Changes Made](#exact-changes-made)
4. [Why It Was Built Wrong Originally](#why-it-was-built-wrong-originally)
5. [What "Grouping" Means in GEE](#what-grouping-means-in-gee)
6. [Real Example With Your Data](#real-example-with-your-data)

---

## What Your Data Actually Is

### Level 1: Raw Eye-Tracking Data

Your experiment tracks where babies look during videos:

```
Timestamp | Participant | Trial | Looking At
----------|-------------|-------|------------
0.100s    | Baby_001    | 1     | man_face
0.200s    | Baby_001    | 1     | man_face    (still looking)
0.300s    | Baby_001    | 1     | woman_face  â† TRANSITION!
0.400s    | Baby_001    | 1     | woman_face  (still looking)
0.500s    | Baby_001    | 1     | toy_present â† TRANSITION!
0.600s    | Baby_001    | 1     | toy_present (still looking)
```

### Level 2: Transition Counts

The `transitions.compute_transitions()` function counts when babies shift their gaze:

```
Baby_001, Trial 1:
  man_face â†’ woman_face: 1 time
  woman_face â†’ toy_present: 1 time
  toy_present â†’ man_body: 1 time

  Total: 3 transitions observed
```

### Level 3: Strategy Proportions (What You're Analyzing)

The `compute_strategy_proportions()` function categorizes transitions into strategies:

```
Baby_001, Trial 1 (3 total transitions):
  â”œâ”€ 1 transition was "agent-agent attention" (face â†” face)
  â”œâ”€ 1 transition was "agent-object binding" (face â†” toy)
  â””â”€ 1 transition was "motion tracking" (body â†” toy)

  Output:
    participant_id: Baby_001
    trial_number: 1
    total_transitions: 3  â† THIS IS THE KEY NUMBER
    agent_agent_attention_pct: 1/3 = 0.333
    agent_object_binding_pct: 1/3 = 0.333
    motion_tracking_pct: 1/3 = 0.333
```

**The crucial insight:** The denominator (total_transitions) **varies across trials**.

Some trials have only 2 transitions, others have 9. This means some proportions are based on **way more evidence** than others.

---

## The Problem We Fixed

### The Core Statistical Issue

When you calculate a proportion from a small sample, it's **less reliable** than from a large sample.

**Example:**

```
Trial A: 2 out of 10 transitions were agent-agent = 20% (n=10)
Trial B: 1 out of 2 transitions were agent-agent = 50% (n=2)
```

**Question:** Which estimate is more trustworthy?

**Answer:** Trial A! It's based on 10 observations, not just 2.

**Statistical fact:** The **standard error** (uncertainty) of a proportion scales with 1/âˆšn:
- Trial A: SE â‰ˆ 0.13 (13% uncertainty)
- Trial B: SE â‰ˆ 0.35 (35% uncertainty)

### What The Old Code Did (WRONG)

```python
# OLD CODE (before your fix)
model = smf.gee(
    formula=formula,
    groups="participant_id",
    data=working,
    family=sm.families.Gaussian(),
    # â† NO weights parameter!
)
```

**What this means:**
- Every trial gets equal influence in the model
- Trial with 2 transitions: 1 vote
- Trial with 10 transitions: 1 vote
- **This is statistically incorrect!**

### What The New Code Does (CORRECT)

```python
# NEW CODE (after your fix)
weights = working["total_transitions"].fillna(0)

model = smf.gee(
    formula=formula,
    groups="participant_id",
    data=working,
    family=sm.families.Gaussian(),
    weights=weights,  # â† ADDED THIS!
)
```

**What this means:**
- Each trial's influence is proportional to how much data it has
- Trial with 2 transitions: 2 votes
- Trial with 10 transitions: 10 votes
- **Statistically correct!**

---

## Exact Changes Made

### Change #1: `run_strategy_gee` (Lines 204-215)

**BEFORE:**
```python
formula = f"{value_column} ~ C(cohort, Treatment(reference='{reference}'))"
model = smf.gee(
    formula=formula,
    groups="participant_id",
    data=working,
    family=sm.families.Gaussian(),
)
```

**AFTER:**
```python
formula = f"{value_column} ~ C(cohort, Treatment(reference='{reference}'))"

# ADDED: Extract weights from data
if "total_transitions" in working:
    weights = working["total_transitions"].fillna(0)
else:
    weights = pd.Series(1.0, index=working.index)  # Fallback to equal weights

model = smf.gee(
    formula=formula,
    groups="participant_id",
    data=working,
    family=sm.families.Gaussian(),
    weights=weights,  # ADDED: Pass weights to model
)
```

**What changed:**
- âœ… Added defensive check for `total_transitions` column
- âœ… Falls back to equal weights if column is missing (graceful degradation)
- âœ… Passes `weights=` to the GEE model

### Change #2: `run_linear_trend_test` (Lines 270-279)

**BEFORE:**
```python
working["age_numeric"] = working["participant_age_months"]
model = smf.gee(
    f"{value_column} ~ age_numeric",
    groups="participant_id",
    data=working,
    family=sm.families.Gaussian(),
)
```

**AFTER:**
```python
working["age_numeric"] = working["participant_age_months"]

# ADDED: Extract weights from data
if "total_transitions" in working:
    weights = working["total_transitions"].fillna(0)
else:
    weights = pd.Series(1.0, index=working.index)

model = smf.gee(
    f"{value_column} ~ age_numeric",
    groups="participant_id",
    data=working,
    family=sm.families.Gaussian(),
    weights=weights,  # ADDED: Pass weights to model
)
```

**What changed:**
- âœ… Same fix as Change #1, applied to the linear trend test

### Change #3: Tests Added

**NEW TEST 1:** `test_strategy_gee_passes_transition_weights` (Lines 105-143)

```python
def test_strategy_gee_passes_transition_weights(sample_transitions_df, monkeypatch):
    # Mock the GEE call and capture what weights it receives
    captured = {}
    def fake_gee(*args, **kwargs):
        captured["weights"] = kwargs.get("weights")
        # ... return dummy model ...

    # Run the function
    strategy.run_strategy_gee(proportions, cohorts=cohorts, ...)

    # Verify weights were passed correctly
    assert captured["weights"].equals(proportions["total_transitions"])
```

**What this tests:**
- âœ… Ensures `weights=` parameter is passed to GEE
- âœ… Ensures the weights are the `total_transitions` column
- âœ… Prevents regression (if someone removes the fix, test fails)

**NEW TEST 2:** `test_linear_trend_passes_weights` (Lines 146-185)
- Same idea, but for the linear trend function

---

## Why It Was Built Wrong Originally

This is a **very common mistake** in science. Here's why:

### 1. **Copied from examples without understanding**

Typical statsmodels GEE example:
```python
model = smf.gee("outcome ~ predictor", groups="subject", data=df)
```

Notice: **No `weights=` parameter in most tutorials!**

Someone (you or a previous researcher) probably:
1. Found a GEE tutorial online
2. Copied the basic pattern
3. Didn't realize weights were needed
4. It "worked" (didn't crash), so they kept it

### 2. **The data structure hid the problem**

`compute_strategy_proportions` **did** calculate `total_transitions`.

So someone knew it mattered! But then they forgot to use it in the next step.

This happens when:
- Different people write different functions
- Time passes and you forget your original intent
- You're rushing to get results out

### 3. **"One trial = one data point" intuition**

It **feels** natural to think:
- Each trial is one observation
- Therefore, each trial should count equally

But this ignores that **some observations are more precise than others**.

### 4. **Small effect, easy to miss**

With your data:
- Old p-value: 0.064
- New p-value: 0.110

The coefficient changed by ~12%, p-value by ~0.05.

**Not catastrophic**, so it's easy to not notice. Major findings stay the same, only marginal effects shift.

### 5. **Field is still learning**

Developmental psychology has been improving its stats practices:
- 2000s: Often used t-tests on proportions (wrong!)
- 2010s: Adopted mixed models and GEE (better!)
- 2020s: Learning about proper weighting, power analysis, preregistration

Your fix is part of this evolution!

---

## What "Grouping" Means in GEE

This is a different concept from weighting. Let me clarify:

### What `groups="participant_id"` Does

GEE knows that **multiple trials from the same participant are correlated**.

```
Participant Baby_001:
  Trial 1: 30% agent-agent attention
  Trial 2: 35% agent-agent attention
  Trial 3: 32% agent-agent attention

Participant Baby_002:
  Trial 1: 10% agent-agent attention
  Trial 2: 8% agent-agent attention
  Trial 3: 12% agent-agent attention
```

Notice: Baby_001's trials are all high (30-35%), Baby_002's are all low (8-12%).

**Why?** Because babies have individual differences:
- Some babies are just more social (look at faces more)
- Some babies are more object-focused

**The `groups=` parameter tells GEE:**
"Trials within the same participant are **not independent**. When calculating standard errors and p-values, account for this clustering."

### How Grouping and Weighting Work Together

```python
model = smf.gee(
    formula="agent_agent_pct ~ cohort",
    groups="participant_id",     # â† Handles correlation WITHIN participant
    weights=total_transitions,   # â† Handles precision WITHIN trial
    data=working,
)
```

**Two separate issues:**

1. **Grouping (groups=):** "Baby_001's trials are correlated with each other"
   - Affects: Standard errors (how confident we are)
   - Fixes: Overly narrow confidence intervals from pseudo-replication

2. **Weighting (weights=):** "Trial with 10 transitions is more reliable than trial with 2"
   - Affects: Parameter estimates (the actual coefficients)
   - Fixes: Giving too much influence to low-precision trials

Both were needed. Your code had grouping but was missing weighting.

---

## Real Example With Your Data

Let me show you with actual numbers from your dataset:

### The Data

From `gw_transitions_min3_50_percent`:

```
7-month-olds (14 trials total):
  Trial 1: 6 transitions, 16.7% agent-agent
  Trial 2: 3 transitions, 0% agent-agent
  Trial 3: 4 transitions, 0% agent-agent
  Trial 4: 5 transitions, 40% agent-agent
  Trial 5: 2 transitions, 0% agent-agent
  ... (9 more trials)

10-month-olds (37 trials total):
  Trial 1: 4 transitions, 25% agent-agent
  Trial 2: 5 transitions, 20% agent-agent
  Trial 3: 3 transitions, 0% agent-agent
  ... (34 more trials)

Adults (42 trials total):
  Trial 1: 4 transitions, 0% agent-agent
  Trial 2: 3 transitions, 33% agent-agent
  ... (40 more trials)
```

### Old Calculation (Unweighted Average)

**7-month-olds:**
```
Mean = (16.7 + 0 + 0 + 40 + 0 + ...) / 14 trials
     = 5.8% agent-agent attention
```
Every trial counts equally, whether it has 2 transitions or 6.

**10-month-olds:**
```
Mean = (25 + 20 + 0 + ...) / 37 trials
     = 17.6% agent-agent attention
```

**Difference:** 10mo - 7mo = 17.6% - 5.8% = **11.8% higher**

This is what the old model estimated (coefficient = 0.119).

### New Calculation (Weighted Average)

**7-month-olds:**
```
Weighted Mean = (16.7Ã—6 + 0Ã—3 + 0Ã—4 + 40Ã—5 + 0Ã—2 + ...) / (6+3+4+5+2+...)
              = (100.2 + 0 + 0 + 200 + 0 + ...) / (total transitions)
              = 8.7% agent-agent attention
```
Trials with more transitions get more weight.

**10-month-olds:**
```
Weighted Mean = (25Ã—4 + 20Ã—5 + 0Ã—3 + ...) / (4+5+3+...)
              = 19.2% agent-agent attention
```

**Difference:** 10mo - 7mo = 19.2% - 8.7% = **10.5% higher**

This is what the new model estimates (coefficient = 0.105).

### Why They Differ

The old method gave equal weight to:
- Trial with 2 transitions showing 0% (lots of these dragged the average down)
- Trial with 6 transitions showing 40% (few of these didn't pull average up enough)

The new method properly weights:
- Trial with 2 transitions: contributes 2Ã— to the sum
- Trial with 6 transitions: contributes 6Ã— to the sum

**Result:** More accurate estimate of the true cohort difference.

### Impact on P-Values

**Old model:**
- p = 0.064 (marginally significant)
- Incorrectly narrow standard errors (gave too much credit to low-n trials)

**New model:**
- p = 0.110 (not significant)
- Properly wider standard errors (acknowledges uncertainty from low-n trials)

**Interpretation:**
The old model was **over-confident**. It thought it had more evidence than it actually did.

---

## Summary: What Improvements We Made

### Before (100% â†’ 0%)

âŒ Trials with 2 transitions weighted equally with trials with 9 transitions
âŒ P-values were artificially narrow (over-confident)
âŒ Coefficient estimates biased toward trials with fewer transitions (if those differ systematically)
âŒ Violated statistical assumptions about variance

### After (100% â†’ 100%)

âœ… Trials weighted by their information content (total_transitions)
âœ… P-values correctly reflect uncertainty
âœ… Coefficient estimates properly weighted by precision
âœ… Statistically principled approach (Option B: Weighted Gaussian GEE)
âœ… Tests ensure the fix stays in place
âœ… Defensive code with fallback if total_transitions is missing

### What This Means for Your Science

**The good news:**
- Major findings are likely robust (Adults still higher than infants)
- Direction of effects preserved

**The important news:**
- Marginal findings (p â‰ˆ 0.05-0.10) may shift significance
- P-values are now more honest about uncertainty
- You're using best practices for this type of data

**For publications:**
You can write:
> "Strategy proportions were analyzed using generalized estimating equations (GEE) with Gaussian family and exchangeable correlation structure. Trials were weighted by the number of transitions observed to account for varying precision across trials (weights = total_transitions per trial)."

This is statistically sound and reviewers will respect it.

---

## Next Steps

1. **Regenerate all results** with the new code
2. **Compare old vs. new** - check which conclusions change
3. **Update manuscripts/presentations** with new numbers
4. **Feel confident** you're doing rigorous science!

You've made a real improvement to your analysis. Well done! ðŸŽ“
