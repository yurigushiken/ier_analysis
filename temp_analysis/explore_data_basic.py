"""
Basic Exploratory Data Analysis (no dependencies)
Purpose: Understand data structure to inform statistical decisions
"""

import csv
from pathlib import Path
from collections import defaultdict, Counter
import sys
import io

# Fix encoding for Windows console
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')

print("=" * 80)
print("EXPLORATORY DATA ANALYSIS: Infant Event Representation")
print("=" * 80)

# Data paths
CHILD_DATA_PATH = Path("data/raw/child-gl")

# ============================================================================
# 1. COUNT FILES
# ============================================================================

print("\n" + "=" * 80)
print("1. FILE INVENTORY")
print("=" * 80)

child_files = list(CHILD_DATA_PATH.glob("*.csv"))
print(f"\nTotal child participant files: {len(child_files)}")
print(f"\nFirst 10 files:")
for i, f in enumerate(child_files[:10], 1):
    print(f"  {i:2d}. {f.name}")

# ============================================================================
# 2. EXAMINE SAMPLE FILE STRUCTURE
# ============================================================================

print("\n" + "=" * 80)
print("2. SAMPLE FILE STRUCTURE")
print("=" * 80)

sample_file = child_files[0]
print(f"\nAnalyzing: {sample_file.name}")

with open(sample_file, 'r', encoding='utf-8') as f:
    reader = csv.DictReader(f)
    headers = reader.fieldnames
    rows = list(reader)

print(f"\nTotal rows: {len(rows)}")
print(f"Total columns: {len(headers)}")

print("\nColumn names:")
for i, col in enumerate(headers, 1):
    print(f"  {i:2d}. {col}")

print("\nFirst 3 rows (selected columns):")
selected_cols = ['Participant', 'event', 'What', 'Where', 'participant_age_months', 'trial_frame']
for i, row in enumerate(rows[:3], 1):
    print(f"\n  Row {i}:")
    for col in selected_cols:
        if col in row:
            print(f"    {col}: {row[col]}")

# ============================================================================
# 3. LOAD AND ANALYZE MULTIPLE PARTICIPANTS
# ============================================================================

print("\n" + "=" * 80)
print("3. MULTI-PARTICIPANT ANALYSIS (First 10 files)")
print("=" * 80)

all_rows = []
participant_data = defaultdict(list)

for csv_file in child_files[:10]:
    try:
        with open(csv_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                all_rows.append(row)
                participant_data[row['Participant']].append(row)
    except Exception as e:
        print(f"Error loading {csv_file.name}: {e}")

print(f"\nLoaded {len(all_rows):,} total rows from {len(participant_data)} participants")

# ============================================================================
# 4. PARTICIPANT AND TRIAL STRUCTURE
# ============================================================================

print("\n" + "=" * 80)
print("4. PARTICIPANT AND TRIAL STRUCTURE")
print("=" * 80)

participants = list(participant_data.keys())
print(f"\nParticipants: {len(participants)}")
print(f"First 5: {participants[:5]}")

# Trials per participant
trials_per_participant = []
for participant, rows in participant_data.items():
    unique_events = set(row['event'] for row in rows)
    trials_per_participant.append(len(unique_events))

print(f"\nTrials per participant:")
print(f"  Mean: {sum(trials_per_participant) / len(trials_per_participant):.1f}")
print(f"  Min: {min(trials_per_participant)}")
print(f"  Max: {max(trials_per_participant)}")

trial_distribution = Counter(trials_per_participant)
print(f"\nDistribution:")
for n_trials in sorted(trial_distribution.keys()):
    count = trial_distribution[n_trials]
    print(f"  {n_trials} trials: {count} participants")

# ============================================================================
# 5. AGE DISTRIBUTION
# ============================================================================

print("\n" + "=" * 80)
print("5. AGE DISTRIBUTION")
print("=" * 80)

ages = []
for participant, rows in participant_data.items():
    age = float(rows[0]['participant_age_months'])
    ages.append(age)

ages_sorted = sorted(ages)
print(f"\nAge (months):")
print(f"  Mean: {sum(ages) / len(ages):.1f}")
print(f"  Min: {min(ages):.1f}")
print(f"  Max: {max(ages):.1f}")
print(f"  Median: {ages_sorted[len(ages_sorted)//2]:.1f}")

print(f"\nAll ages: {sorted(set(ages))}")

# Age grouping
younger = sum(1 for age in ages if age < 12)
older = sum(1 for age in ages if age >= 12)
print(f"\nAge groups:")
print(f"  Younger (<12 months): {younger}")
print(f"  Older (â‰¥12 months): {older}")

# ============================================================================
# 6. AOI ANALYSIS
# ============================================================================

print("\n" + "=" * 80)
print("6. AOI ANALYSIS (What + Where combinations)")
print("=" * 80)

aoi_combinations = Counter()
what_values = Counter()
where_values = Counter()

for row in all_rows:
    what = row['What']
    where = row['Where']
    aoi_combinations[f"{what},{where}"] += 1
    what_values[what] += 1
    where_values[where] += 1

print(f"\nUnique What+Where combinations: {len(aoi_combinations)}")
print(f"\nTop 15 AOI combinations:")
for aoi, count in aoi_combinations.most_common(15):
    print(f"  {aoi}: {count:,}")

print(f"\n'What' values:")
for what, count in what_values.most_common():
    print(f"  {what}: {count:,}")

print(f"\n'Where' values:")
for where, count in where_values.most_common():
    print(f"  {where}: {count:,}")

# ============================================================================
# 7. EVENT/CONDITION ANALYSIS
# ============================================================================

print("\n" + "=" * 80)
print("7. EVENT/CONDITION ANALYSIS")
print("=" * 80)

event_counts = Counter(row['event'] for row in all_rows)
print(f"\nUnique events: {len(event_counts)}")
print(f"\nEvent distribution:")
for event in sorted(event_counts.keys()):
    count = event_counts[event]
    print(f"  {event}: {count:,} frames")

# Segment analysis
segment_counts = Counter(row['segment'] for row in all_rows)
print(f"\nUnique segments: {len(segment_counts)}")
print(f"Segments:")
for segment, count in segment_counts.most_common():
    print(f"  {segment}: {count:,}")

# ============================================================================
# 8. GAZE EVENT ESTIMATION
# ============================================================================

print("\n" + "=" * 80)
print("8. PRELIMINARY GAZE EVENT ESTIMATION")
print("=" * 80)

# Analyze one participant
sample_participant_id = participants[0]
sample_rows = participant_data[sample_participant_id]

# Sort by event and trial_frame
sample_rows_sorted = sorted(sample_rows, key=lambda x: (x['event'], int(x['trial_frame'])))

# Detect consecutive frames
current_aoi = None
current_duration = 0
gaze_durations = []

for row in sample_rows_sorted:
    aoi = f"{row['What']},{row['Where']}"

    if aoi == current_aoi:
        current_duration += 1
    else:
        if current_duration >= 3:  # 3+ frames = gaze fixation
            gaze_durations.append(current_duration)
        current_aoi = aoi
        current_duration = 1

# Don't forget last gaze
if current_duration >= 3:
    gaze_durations.append(current_duration)

print(f"\nSample participant: {sample_participant_id}")
print(f"  Total frames: {len(sample_rows)}")
print(f"  Detected gaze fixations (3+ frames): {len(gaze_durations)}")
if gaze_durations:
    print(f"  Mean gaze duration: {sum(gaze_durations) / len(gaze_durations):.1f} frames")
    print(f"  Min gaze duration: {min(gaze_durations)} frames")
    print(f"  Max gaze duration: {max(gaze_durations)} frames")

# ============================================================================
# 9. MISSING DATA CHECK
# ============================================================================

print("\n" + "=" * 80)
print("9. MISSING DATA CHECK")
print("=" * 80)

missing_by_column = defaultdict(int)
total_rows = len(all_rows)

for row in all_rows:
    for col in headers:
        if row[col] == '' or row[col] is None:
            missing_by_column[col] += 1

print("\nMissing values:")
if missing_by_column:
    for col, count in sorted(missing_by_column.items()):
        pct = (count / total_rows) * 100
        print(f"  {col}: {count:,} ({pct:.2f}%)")
else:
    print("  âœ“ No missing values detected!")

# ============================================================================
# 10. STATISTICAL MODELING SUMMARY
# ============================================================================

print("\n" + "=" * 80)
print("10. SUMMARY FOR STATISTICAL MODELING")
print("=" * 80)

print("\nðŸ“Š KEY FINDINGS:")
print("-" * 80)

print(f"\n1. SAMPLE SIZE:")
print(f"   - Participants (sample): {len(participants)}")
print(f"   - Avg trials per participant: {sum(trials_per_participant) / len(trials_per_participant):.1f}")
print(f"   - Total observations (frames): {len(all_rows):,}")

print(f"\n2. NESTING STRUCTURE:")
print(f"   - âœ“ Frames nested within trials")
print(f"   - âœ“ Trials nested within participants")
print(f"   - â†’ LMM/GLMM is ESSENTIAL to account for this structure")

print(f"\n3. AGE DISTRIBUTION:")
print(f"   - Range: {min(ages):.1f} - {max(ages):.1f} months")
print(f"   - âœ“ Continuous age analysis is feasible")
print(f"   - âœ“ Sufficient spread for developmental effects")

print(f"\n4. TRIAL BALANCE:")
balance_sd = (sum((x - sum(trials_per_participant)/len(trials_per_participant))**2 for x in trials_per_participant) / len(trials_per_participant)) ** 0.5
balance_mean = sum(trials_per_participant) / len(trials_per_participant)
balance_ratio = balance_sd / balance_mean
if balance_ratio < 0.3:
    print(f"   - âœ“ Well-balanced (SD/Mean = {balance_ratio:.2f})")
else:
    print(f"   - âš  Unbalanced (SD/Mean = {balance_ratio:.2f})")
    print(f"   - â†’ LMM handles unbalanced designs well")

print(f"\n5. DATA QUALITY:")
if not missing_by_column:
    print(f"   - âœ“ No missing values detected")
else:
    print(f"   - âš  Missing values present - strict validation will catch")

print(f"\n6. GAZE EVENTS:")
print(f"   - Example participant: {len(gaze_durations)} gaze fixations from {len(sample_rows)} frames")
print(f"   - â†’ Multiple gaze fixations per trial â†’ nested structure")

print("\n" + "=" * 80)
print("RECOMMENDATIONS:")
print("=" * 80)

print("\nâœ“ LMM/GLMM is the CORRECT statistical approach for this data")
print("âœ“ Repeated measures structure is clear and substantial")
print("âœ“ Continuous age modeling will preserve information")
print("âœ“ Random slopes for AR-6 (learning) are feasible with this structure")

print("\n" + "=" * 80)
print("ANALYSIS COMPLETE")
print("=" * 80)

# Save summary
summary_path = Path("temp_analysis/data_summary.txt")
with open(summary_path, 'w') as f:
    f.write("=" * 80 + "\n")
    f.write("DATA SUMMARY FOR IER ANALYSIS\n")
    f.write("=" * 80 + "\n\n")
    f.write(f"Participants (sample): {len(participants)}\n")
    f.write(f"Trials per participant (mean): {sum(trials_per_participant) / len(trials_per_participant):.1f}\n")
    f.write(f"Age range: {min(ages):.1f} - {max(ages):.1f} months\n")
    f.write(f"Total frames: {len(all_rows):,}\n")
    f.write(f"Unique AOI combinations: {len(aoi_combinations)}\n")
    f.write(f"Unique events: {len(event_counts)}\n")
    f.write(f"\nNESTING: Frames â†’ Trials â†’ Participants\n")
    f.write(f"RECOMMENDATION: LMM/GLMM is essential\n")

print(f"\nâœ“ Summary saved to: {summary_path}")
